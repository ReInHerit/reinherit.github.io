{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<p><img src=\"https://ucarecdn.com/cf4dd5aa-770e-4aa3-b26b-d8e0dcb258f5/\" alt=\"\"></p>\n<p>The purpose of the application is to create an immersive audio experience that can be setup at museums. Immersive audio stations are placed in various areas of a museum, offering visitors an interactive experience. Users that enter in the corresponding areas within the museum, based on their behavior (e.g., based on how many people are in the area, or how active they are), trigger specific sounds that are layered together creating a music composition.</p>\n<p>The code for the application can be accessed from the following GitHub repository link: <a href=\"https://github.com/CYENS/Reinherit-Hadjigeorgakis-Kornesios-Mansion\">https://github.com/CYENS/Reinherit-Hadjigeorgakis-Kornesios-Mansion</a>.</p>\n<p>A Technical Manual for the Immersive Performance, to be used to set up a similar performance can be downloaded as a PDF below.</p>","excerpt":"The purpose of the application is to create an immersive audio experience that can be setup at museums. Immersive audio stations are placed…","frontmatter":{"title":"Immersive and Interactive Music Performance","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"7cbc7084-6711-4ac0-88c6-4e05cc34ae50","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/dad8926d-b220-4847-a923-7bdfa8aca482/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Immersive.20Performances"},"wordCount":{"words":115}}},{"node":{"html":"<p>This application is designed for <strong>cultural smart tourism</strong> and provides functionalities for landmark recognition using <strong>computer vision.</strong> The vision system is able to deal also with large monuments, where only a portion of the landmark is visible, using a specific run-time image augmentation process that is combined with a training-time augmentation.  The smart tourism app is a native <strong>Android app</strong> that provides the basic functionalities. It has been developed as an Android app to overcome the computational limitations of web based systems, that are still not feature complete when using <strong>TensorflowJS</strong> with respect to Tensorflow Lite. Despite this it works also on low and  mid-level devices, since it has been designed to use a set of very fast neural network that can be executed in real-time (based on variations of the MobileNet V3 architecture). The system uses computer vision to recognize <strong>landmarks</strong> and monuments, providing <strong>multimedia information</strong>, and  can be <strong>personalized</strong> to create different tours.</p>\n<p>The <strong>end-user application</strong> is complemented by a set of backend tools that create the set of images needed to recognize landmarks, performing an augmentation that is used both at training time and at test time; these tools are to be used by the creators of the smart guides.</p>\n<p>The vision system implements a <strong>content-based image retrieval (CBIR),</strong> using techniques that are completely different, because of the nature of the task that is addressing landmarks, from that of the Smart Lens app. In fact when considering the recognition of landmarks a common case is that the user does not frame with the lens the whole object, and that the object itself has <strong>many different and possibly diverse views</strong>. To this end when creating the guide the curators must use a <strong>large number of images.</strong> To cope with the variability of the point of view of the users each image is further split in different parts and zoomed in and out versions are created, as shown in the following figures. All these augmentations result in the creation of a very <strong>large dataset of images</strong> representing the landmarks.</p>\n<p><img src=\"https://ucarecdn.com/96cde401-f599-4fdd-84a0-da1ce1b0d742/\" alt=\"image splitting augmentation\" title=\"image splitting augmentation\"></p>\n<blockquote>\n<p><em>Image splitting augmentation</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/c7afd79a-b476-490b-8754-3bd4d3ec41f9/\" alt=\"image zooming augmentation\" title=\"image zooming augmentation\"></p>\n<blockquote>\n<p><em>Image zooming augmentation</em></p>\n</blockquote>\n<p>To cope with the large number of images, these are indexed using <strong>FAISS</strong>, an indexing library that allows to perform approximate nearest neighbour retrieval, thus reducing the number of actual image comparisons that are needed to determine the landmark that is framed. This need is exacerbated by the fact that we also implement a test-time augmentation, i.e. the image that is taken with the mobile phone is used to generate two zoomed versions, and each image is further split into 3x3. This test-time augmentation is used <strong>to improve the performance of the CBIR system</strong> in terms of accuracy. For each split of each image obtained during the use of the application is computed a descriptor using the MobileNet network, and using FAISS a nearest neighbor image of the database is retrieved and ranked in terms of visual similarity, selecting the 3 most similar images, as shown in the following figure. A K-NN classifier is used to recognize the landmark based on these retrieved images.</p>\n<p><img src=\"https://ucarecdn.com/b2207fb6-2dbd-4560-b4e0-fef415fa1535/\" alt=\"smart tourism app: landmark recognition\" title=\"smart tourism app: landmark recognition\"></p>\n<blockquote>\n<p><em>Smart tourism app: landmark recognition</em></p>\n</blockquote>\n<p>To test the performance of the computer vision system, its accuracy performance was tested on the difficult dataset Paris Revisited dataset, developed by the University of Oxford. This dataset is composed of 5000 images of landmarks; we used ⅔ for training and ⅓ for test.</p>\n<p><img src=\"https://ucarecdn.com/51a9476e-b72d-4a5a-ac99-53d77a3b6262/\" alt=\"Revisited Paris dataset\" title=\"Revisited Paris dataset\"></p>\n<blockquote>\n<p><em>Revisited Paris dataset: for each query image is provide a set of good (dark green), medium (light green) and hard (yellow) images that are associated. A CBIR system should provide the results in these categories in this order.</em></p>\n</blockquote>\n<p>Considering the <strong>computational costs</strong> of the variants of MobileNet V3 tested, the smaller version allows to compute the visual features in 0.006 seconds, making it suitable for low-end devices that do not have much computational capabilities. This at the cost of losing 5 percent points in terms of accuracy. On the other hand mid and high-end devices can use a medium and a large network version, that require 0.023 and 0.038 seconds. Thanks to the use of FAISS, the search of similar images takes a negligible amount of time (0.004 seconds). Once the set of  similar images has been obtained the result of the K-NN classifier is instantaneous. The accuracy of the large network is ~76%, while the medium version obtains ~73% and the smaller one 68%. Using the test-time augmentation improves the performance by 6 points, thus reaching up to 82%, showing the <strong>benefit of this technique.</strong></p>\n<p>The following figure shows screenshots of the <strong>application</strong>, with additional debug information in the first two images, and the recognition of a landmark with the associated information.</p>\n<p><img src=\"https://ucarecdn.com/975ec430-2cc7-4bb2-b6e2-0069861b7a20/\" alt=\"Active exploration: Users interact with the app getting info from the lists of suggested tours, the suggestion adapts based on the clicks of the user and the description of the landmark.  Users are directed to the landmarks of interest and get suggestion on other relevant destinations.\" title=\"Active exploration: Users interact with the app getting info from the lists of suggested tours, the suggestion adapts based on the clicks of the user and the description of the landmark.  Users are directed to the landmarks of interest and get suggestion on other relevant destinations.\"></p>\n<blockquote>\n<p><em>Active exploration: Users interact with the app getting info from the lists of suggested tours, the suggestion adapts based on the clicks of the user and the description of the landmark.  Users are directed to the landmarks of interest and get suggestion on other relevant destinations.</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/8be5bece-e57c-4192-b1bc-47e7d62e690d/\" alt=\"Active exploration: Users are alerted about their proximity to an interesting landmark  Users can take a photo of something that attracts their interest and get related info\" title=\"Active exploration: Users are alerted about their proximity to an interesting landmark  Users can take a photo of something that attracts their interest and get related info\"></p>\n<blockquote>\n<p><em>Active exploration: Users are alerted about their proximity to an interesting landmark  Users can take a photo of something that attracts their interest and get related info</em></p>\n<h3>Source code</h3>\n<p>The source code of the app is available on the Github of ReInHerit: <a href=\"https://github.com/ReInHerit/SmartTourism\">https://github.com/ReInHerit/SmartTourism</a></p>\n<p><strong>Link to Google Play Store</strong></p>\n<ul>\n<li><a href=\"https://play.google.com/store/apps/details?id=org.reinherit.lite.examples.classification\">Smart Tourism Florence</a></li>\n<li><a href=\"https://play.google.com/apps/testing/org.reinherit.lite.examples.classification.athens\">Smart Tourism Athens (testing)</a></li>\n<li>Smart Tourism Nicosia (coming soon)</li>\n</ul>\n<p>\n        <div class=\"embedVideo-container\">\n            <iframe\n              title=\"\"\n              width=\"560\"\n              height=\"316\"\n              src=\"https://www.youtube.com/embed/Mlfbmtp1LKk?rel=0\"\n              class=\"embedVideo-iframe\"\n              style=\"border:0\"\n              \n              loading=\"eager\"\n              allowfullscreen\n\t      sandbox=\"allow-same-origin allow-scripts allow-popups\"\n            ></iframe>\n        </div></p>\n</blockquote>","excerpt":"This application is designed for cultural smart tourism and provides functionalities for landmark recognition using computer vision. The…","frontmatter":{"title":"Smart Tourism App ","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"437c8b14-f2c8-4fe6-9162-104d4abbb5b6","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/4f495df3-8f37-4899-a5c0-6df7d87f6baa/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Smart.20Tourism.20App"},"wordCount":{"words":969}}},{"node":{"html":"<p>Smart Lens is a <strong>web application</strong> that can be used to create an <strong>interactive visual guide</strong> that <strong>recognizes artwork details</strong>. The application can be executed on a <strong>mobile device</strong> like a <strong>mobile phone</strong> or a <strong>tablet</strong>.</p>\n<p>The user can employ his mobile phone (or tablet) as if it was a lens to inspect and analyze details of interest that are recognized using a variety of computer vision techniques (CBIR, classification or object detection). As <strong>details are recognized</strong> the app <strong>provides multimedia information on</strong> them. The app can be used as a smart guide to a collection or for a set of artworks that are of particular relevance and that can necessitate a thorough analysis. The type of interaction elicited from a user is more active than that of a typical guide, e.g. based on QR codes, since it calls for an active analysis of the exhibition. An additional benefit is that it is possible to collect <strong>anonymized information</strong> on the details and artworks of interest, providing the curators with <strong>statistics</strong> and information on how the exhibitions are experiences by visitors. </p>\n<p>The app can use <strong>three different computer vision techniques</strong>, each one eliciting a different behaviour from the user. The application allows the recognition of artwork details using the folllowing  <strong>alternative methods:</strong> </p>\n<ul>\n<li><strong>Content-based Image retrieval (CBIR)</strong> </li>\n<li><strong>Classification</strong> </li>\n<li><strong>Object detection</strong> </li>\n</ul>\n<p>The <strong>Content-based Image retrieval (CBIR)</strong> mechanism allows the recognition of the works and their details through the comparison between the visual features extracted from a dataset of images (the collection of details stored in a database, i.e. in the guide) and those obtained from the frame produced by the user's camera.  To allow the recognition of the details of the works, the features have been extracted not only from the entire work but also from some of its parts obtained through a rigid subdivision as shown in the following figure.</p>\n<p><img src=\"https://ucarecdn.com/8893d914-6f5c-43be-9ee7-2260c65ea995/\" alt=\"image subdivision/augmentation used for CBIR \" title=\"image subdivision/augmentation used for CBIR \"></p>\n<blockquote>\n<p><em>Image subdivision/augmentation used for CBIR</em></p>\n</blockquote>\n<p>The recognition takes place on the Javascript side by extracting the features on the camera frame with the same network used on the backend side and calculating the minimum distance compared to those present in the database. To avoid that an image is always returned even when a point of interest is not framed, a specific maximum distance has been defined for each work or detail, beyond which no result is returned; i.e. there’s a maximum distance for the nearest neighboring image. </p>\n<p>This second approach allows the recognition of the framed detail / work through a classification network on which a fine-tuning operation has been performed on the paintings and their details present in the dataset. This requires, differently from the previous approach, a retraining of the neural network; it must be noted that typically this retraining can be performed even using CPUs. </p>\n<p>In this version, therefore, the <strong>classification</strong> of the entire frame is performed, which is identified with the most probable class only if the degree of confidence is greater than a specific threshold for each image. </p>\n<p>In this version, the recognition is entrusted to a network for <strong>Object Detection</strong> on which fine tuning has been performed on the details of the work. This retraining is more expensive than the previous one and requires a GPU. In this case, only the details within the work are identified by taking the most probable bounding-box (i.e. a rectangle that most probably contains the object of interest) for each class provided that its degree of confidence exceeds a specific threshold for each detail. </p>\n<p>The interface is entirely made in HTML5 and CSS and has a fluid behavior adapting to the size of any screen (tablet or smartphone).  It is dynamically modified through Javascript scripts that allow you to show the results of the matches with the images framed by the camera in real time. </p>\n<p>In the camera view, the work or detail is framed and the recognition operation is carried out in real time.  MobileNetV3 networks were used for Retrieval and classification and SSD/MobilNetV3 for Detection. This view remains hidden until the networks to be used have been loaded correctly as well as the data from the server. </p>\n<p>For each work or detail is provided a textual description, the relative image and any audio and video content.  If an audio file is not available, the application will allow the text-to-speech of the textual.</p>\n<p><img src=\"https://ucarecdn.com/cd3b0ef9-2290-4ee4-b779-883dac74db37/\" alt=\" example of details recognition using the object detection approach: the boxes highlight which part of the image are related to the details. Clicking on the detail icons takes the specific information\" title=\" example of details recognition using the object detection approach: the boxes highlight which part of the image are related to the details. Clicking on the detail icons takes the specific information\"></p>\n<blockquote>\n<p> <em>Example of details recognition using the object detection approach: the boxes highlight which part of the image are related to the details. Clicking on the detail icons takes the specific information</em></p>\n</blockquote>\n<p>The following figure shows other examples of how the details recognized using the computer vision system implementing the object detection capabilities are shown.</p>\n<p><img src=\"https://ucarecdn.com/017ced9e-adbd-4350-8b87-32a168d27b30/\" alt=\"Smart Lens - examples of details recognition and information presentation (images courtesy of MCA, Graz Museum, BoCCF)\" title=\"Smart Lens - examples of details recognition and information presentation (images courtesy of MCA, Graz Museum, BoCCF)\"></p>\n<blockquote>\n<p><em>Smart lens - examples of details recognition and information presentation (images courtesy of MCA, Graz Museum, BoCCF)</em></p>\n</blockquote>\n<p>Additional work has been done to improve the training of the neural networks that implement object detection and image classification capabilities to the app. This is due to the fact that typically these types of networks are typically trained and fine-tuned on large numbers of classes of images and objects; in this context of artwork details instead we have to deal with entities that are by definition unique. A set of two backend tools has been developed to augment the training data required to implement the fine tuning of the networks. The system has been implemented in Python and from a small set of images that are taken by the museum curators, it generates a large number of variations altering the perspective of the scene, the position and point of view of the objects, their color and luminance to simulate different types of lenses and image sensors, simulating noise and dirt on the lens, rotating the objects to account for users that employ their device in landscape or portrait orientations; examples of such alterations is shown in the following figure. Since the creation of such augmented datasets is time consuming the code has been parallelized to greatly reduce the time needed to train the networks.</p>\n<p><img src=\"https://ucarecdn.com/e67a915c-ad60-4a95-916f-cbfdb6340c86/\" alt=\" Smart lens - examples of augmentations of the training data used to recognized details of the Cherry Rage (images courtesy of Graz Museum)\" title=\" Smart lens - examples of augmentations of the training data used to recognized details of the Cherry Rage (images courtesy of Graz Museum)\"></p>\n<blockquote>\n<p> <em>Smart lens - examples of augmentations of the training data used to recognized details of the Cherry Rage (images courtesy of Graz Museum)</em></p>\n</blockquote>\n<p>To ease the use of the training system by non-expert users, a new web-based application has been developed, integrating also the annotation functionalities required to let curators select the details of artworks from a collection. This system uses Javascript to implement the annotation interface and to manage the training of the networks, and Python with Flask and Tensorflow on the backend to run the augmentation and training tasks. Training is launched as an asynchronous process, so that users can continue to work in the interface.</p>\n<p>The <strong>architecture</strong> of the system is the following:</p>\n<p><img src=\"https://ucarecdn.com/41aebc63-b8c3-4907-9c0d-098ed7a95cb9/\" alt=\"Smart Lens - architecture of the neural network training system\" title=\"Smart Lens - architecture of the neural network training system\"></p>\n<blockquote>\n<p><br>\n<em>Smart lens - architecture of the neural network training system</em></p>\n</blockquote>\n<p><br>\nThe system also allows to run a trained network on a set of images to provide an immediate feedback to the user to understand if there's a need for further training.</p>\n<p>Examples of the interface of the system are shown in the following images. The system allows users to create their sets of concepts and details (the figure in the following shows the sets of COCO dataset concepts used to test the system).</p>\n<p><img src=\"https://ucarecdn.com/4ec0b013-79fe-4788-81ab-2c25ed9b924a/\" alt=\"Smart lens - training system gallery interface\" title=\"Smart lens - training system gallery interface\"></p>\n<blockquote>\n<p><br>\n<em>Smart lens - training system gallery interface</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/248be84e-b2e5-4669-9171-73bfa68986b0/\" alt=\"Smart lens -training system - detail of the widget for selection of neural network\" title=\"Smart lens -training system - detail of the widget for selection of neural network\"></p>\n<blockquote>\n<p><em>Smart lens -training system - detail of the widget for selection of neural network</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/4b097976-ac53-4d09-b888-e0dcfc91c53a/\" alt=\" Smart lens - training system - creation of the annotations of details and metadata for an artwork\" title=\" Smart lens - training system - creation of the annotations of details and metadata for an artwork\"></p>\n<blockquote>\n<p> <em>Smart lens - training system - creation of the annotations of details and metadata for an artwork</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/770ed318-ebe5-4699-91ae-3263625b062c/\" alt=\"Smart lens - examples of automatic (red) and manual (greeen) annotation of details\" title=\"Smart lens - examples of automatic (red) and manual (greeen) annotation of details\"></p>\n<blockquote>\n<p><em>Smart lens - examples of automatic (red) and manual (greeen) annotation of details</em></p>\n</blockquote>\n<p>To speed up the annotation process it is possible to use a pre-trained network, as shown in the above figure, where manual annotation is highlighted in green and the automatica results are highlighted in orange.</p>\n<p>The tool can be used, apart fro training the network used in Smart Lens, also to train networks for recognition of objects and visual concepts in cultural heritage in general</p>\n<p><strong><br>\nUsage example</strong> </p>\n<p>There is a need to use a server to host the backend and the web app itself. A QR code can be used to avoid typing the URL of the web apps. </p>\n<p><strong>Guidelines for reuse</strong> </p>\n<p>The application is composed of a backend that maintains information on the details of the artworks. The first step for reuse is to create the guide itself in all the languages that the museum wants to support. </p>\n<p>The types of interaction vary according to the techniques used to recognize the details: using object detection it is possible to analyze artworks also from afar, getting hints at what parts of the artwork are the “hotspots” for which the information is provided. Using the classification network the user must inspect the artwork more thoroughly, mimicking more the use of a magnifying lens. Using CBIR it is possible to create a user experience between those of object detection and classification, reducing the time to create a dataset (no neural network must be retrained) but at the possible cost of having a reduced recognition performance in case of similar artworks or too similar details. </p>\n<p>The app can be used also to perform artwork recognition, without considering the details.</p>\n<h3>Source code</h3>\n<p>The source code of the app is available on the Github of ReInHerit: <a href=\"https://github.com/ReInHerit/SmartLens-app\">https://github.com/ReInHerit/SmartLens-app</a></p>\n<p>The source code of the app used to fine-tune the neural networks us available on the Github of ReInHerit: <a href=\"https://github.com/ReInHerit/reinherit-webnet-trainer\">https://github.com/ReInHerit/reinherit-webnet-trainer</a></p>\n<h3>Demo SmartLens</h3>\n<p>Try the <strong>Smart Lens Tester</strong> at the following link:<br>\n<a href=\"https://reinherit-smartlens-tester.herokuapp.com/\">https://reinherit-smartlens-tester.herokuapp.com</a></p>","excerpt":"Smart Lens is a web application that can be used to create an interactive visual guide that recognizes artwork details. The application can…","frontmatter":{"title":"Smart Lens","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"beae4f2a-8f09-4de1-949a-398f8b3626b3","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/52c6f7ac-f15b-4d79-a56c-73f1397bf5de/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Smart.20Lens"},"wordCount":{"words":1684}}},{"node":{"html":"<p>This is an application designed for <strong>historical video</strong> archive professionals that allows to restore analog videos using an <strong>innovative neural network</strong> capable of coping with severe degradations such as tape mistracking. Neural Network AI technology that uses multi - frame approach to restore videos and is able to deal with severe tape mistracking, which results in complete scrambled frames.  Historical videos constitute an important part of the cultural heritage of a society.  This novel architecture for restoring degraded real-world analog videos coming from historical archives. A <strong>web app</strong> lets users <strong>upload videos</strong> with similar system-intrinsic and aging-related types of degradations <strong>and download the restored versions.</strong></p>\n<p>The availability of this video content often is hampered by numerous artifacts and degradations due to technological limitations and aging of the recording support that limit its distribution and fruition by the general public. Normally the <strong>restoration</strong> of these videos is conducted frame by frame by experienced archivists with commercial solutions, thus at <strong>great economic and time cos</strong>t. Analog videos of historical archives often contain severe visual degradation due to the deterioration of their tape supports that require costly and slow manual interventions to recover the original content. We are working on a novel neural network that uses a multi-frame approach and is able to deal also with severe tape mistracking, which results in completely scrambled frames. The network can be used to reduce also other types of tape defects that are less severe than tape mistracking. We think that it may also work on films to reduce scratches and mold. The novelty is in the neural network that reduces these artifacts.</p>\n<p>Currently available neural networks for digital video restoration are not capable of addressing severe video degradation. Some works tried to restore historical video archives more rapidly and without human aid. For example, <em>DeOldify</em> is an open-source tool for old film restoration addressing in particular colorization of black and white movies. The scientific community working on CV and multimedia has been active, in very recent years, to develop novel solutions for video restoration and this new system has been demonstrated at the ACM Multimedia 2022.</p>\n<p>Development of this smart video application has been carried on in collaboration with the <em>Historical Archive of Istituto Luce</em>, which has evaluated the results and provided use cases and test materials.  This  is an Italian society responsible for the preservation and distribution of the Archivio Storico Luce, the largest Italian historical video archive dating from throughout the 1900s and comprising a variety of sources, providing some analog videos from this archive. These videos present several system intrinsic and aging-related types of degradations typical of analog video tapes. An example of such degradation is shown in the following figure. This poses a severe risk for the long term maintenance of these types of materials and in the future similar archives risk losing tens of years of archive materials, up to the years in which digital video recording became the common storage support. </p>\n<p>Analyzing the existing approaches for video restoration, they focus on standard structured defects such as scratches and cracks, so they are not capable of restoring the particular types of artifacts that analog video restoration requires. Unfortunately, when considering real-world archive videos, there is no clean high-quality version of them to use as ground truth for supervised learning. Consequently we created a synthetic dataset as similar as possible to the real-world videos to train and evaluate our system. </p>\n<p>Starting from high-quality videos of the Harmonic dataset we used Adobe After Effects to randomly add several types of degradations, such as: </p>\n<ul>\n<li> Gaussian noise, resembling the tape noise that is typical of analog videos; </li>\n<li> white artifacts simulating tape dropouts; </li>\n<li>cyan, magenta and green horizontal lines resembling chroma fringing </li>\n<li>horizontal displacements, similar to tape mistracking artifacts; this is the most complex error that can be encountered. </li>\n</ul>\n<p>As with the real-world videos, all these artifacts vary over time and occur with different intensity, positions and combinations for each frame. The following figure shows an example of the synthetic dataset created to develop the restoration app. On the left the high quality frame, on the right the version with the artifacts generated using After Effects. The combination of frames is used to train the neural network.</p>\n<p><img src=\"https://ucarecdn.com/5b7f46b1-3cec-4e4d-889d-64245982146d/\" alt=\"Example of synthetic training dataset\" title=\"Example of synthetic training dataset\"></p>\n<blockquote>\n<p><em>Example of synthetic training dataset. Left) original high-quality image; Right) degraded version created with Adobe AE. The network is trained to obtain the image on the left from the image on the right, used as input</em></p>\n</blockquote>\n<p>The novel neural network was designed using Transformer networks and processing a group of consecutive frames so that the restoration of a ruined frame is helped by the contextual data provided by the preceding and following ones (even if they are degraded as well). Given the complexity of the task the computation cannot perform in real-time and must be executed as a batch process. </p>\n<p>The performance of this new method was measured using three standard full-reference visual quality metrics: 1) PSNR; 2) SSIM; 3) LPIPS. The quantitative results are shown in the table below. For a fair comparison, DeOldify was re-trained from scratch using our new training data. The new model performed best by a wide margin.</p>\n<p><img src=\"https://ucarecdn.com/cd5a0318-39cd-4b08-adae-8d160ba73a50/\" alt=\" Comparison of the ReInHerit restoration  \" title=\"Comparison of the ReInHerit restoration \"></p>\n<blockquote>\n<p> <em>Comparison of the ReInHerit restoration app w.r.t. DeOldify. Higher values of PSNR and SSIM are better. Lower values of LPIPS are better. Best results are highlighted in bold.</em></p>\n</blockquote>\n<p>The following figure shows examples of the results obtained by the neural network. On the left of each image is shown the input frame, on the right the desired output and in the middle the actual output of the network. These examples have been obtained from the synthetic dataset since it is the only way to have “gold standard” frames. </p>\n<p><img src=\"https://ucarecdn.com/051c0e7a-5798-49ec-9430-d0979b990cb1/\" alt=\"Examples of video frames restoration\" title=\"Examples of video frames restoration\"></p>\n<blockquote>\n<p> <em>Examples of video frames restoration: left) input: degraded frames; middle) output of the network; right) ideal result, i.e. original high quality frame used to produce the degraded version.</em></p>\n</blockquote>\n<p>When applying the system to real world video archives we cannot rely on high-quality reference images, they simply do not exist and the degraded frames are what is available in the archive. The following figures show what happens when the video restoration system is applied in a real-world usage scenario, showing an example of materials provided by Istituto Luce. </p>\n<p><img src=\"https://ucarecdn.com/5ccc8405-a26f-47ff-a7fd-0a52d8a341eb/\" alt=\"Example of results\" title=\"Example of results\"></p>\n<blockquote>\n<p><em>Example of results obtained by smart video restoration app on real-world archive video. Video courtesy of Istituto Luce.</em></p>\n</blockquote>\n<p>The smart video restoration app is <strong>composed of two parts:</strong></p>\n<ul>\n<li>the <strong>backend</strong>, implemented using PyTorch provides the computer vision functionalities that restore the video using a novel neural network architecture, these CV services are provided to the front-end by a REST API implemented in Flask;</li>\n<li>The <strong>front-end i</strong>s a web app accessible through a browser that allows users to submit degraded videos, start their restoration and then download the restored versions. Screenshots of the web app are presented in the following, showing the whole process to restore a video.</li>\n</ul>\n<p><img src=\"https://ucarecdn.com/cf33d44c-2b65-4f03-ba33-8229684e1f97/\" alt=\"upload a file\" title=\"upload a file\"></p>\n<blockquote>\n<p><em>The user can upload a file or choose one of the given examples</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/7ada4b4d-ce2d-41dc-a010-9e114c7ad5c4/\" alt=\"Overview of the chosen video\" title=\"Overview of the chosen video\"></p>\n<blockquote>\n<p><em>An overview of the chosen video provides some information about it, the user can check if the metadata is correct and then can start the restoration process</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/b14c796b-5436-4d9c-b7bb-1cd01e05dfae/\" alt=\"End of the restoration process \" title=\"End of the restoration process \"></p>\n<blockquote>\n<p><em>When the restoration process ends, the user can see and download the restored video and the comparison with the degraded one.</em> </p>\n</blockquote>\n<p>Since the restoration process is computationally expensive, the CV component must be executed on a server with appropriate GPU. It is also possible to use GPU servers commonly available from cloud providers such as Amazon AWS, Google Cloud, Microsoft Azure. </p>\n<p>The system has been further improved, reconsidering the design of the neural network used to restore videos. A new network was designed based on a Swin-UNet, the architecture used in the first version of the application that exploits both neighboring and reference frames by means of a novel Multi-Reference Spatial Feature Fusion block to restore corrupted frames.  CLIP was employed for zero-shot artifact detection to select the cleanest frames of each video as references. Finally,  contrastive loss was extended to a multi-frame setting to make the results more perceptually accurate. Both quantitative and qualitative experiments show the effectiveness of this novel approach when compared with other state-of-the-art methods. The idea is that even corrupted videos contain some relatively good frames that could be selected based on their quality, and use their visual content to provide additional guidance to the neural network that is performing the restoration. A new scientific paper that describes the novel system is currently under submission  at one of the foremost conferences on computer vision (International Conference on Computer Vision, 2023).</p>\n<h3>Source code</h3>\n<p>The source code of the app is available on Github: <a href=\"https://github.com/miccunifi/analog-video-restoration\">https://github.com/miccunifi/analog-video-restoration</a></p>\n<p>A new and more capable version of the app is available here: <a href=\"https://github.com/miccunifi/TAPE\">https://github.com/miccunifi/TAPE</a></p>\n<p>Code for a new metric to evaluate video quality in a no-reference setting (useful for restoration of archive materials) is available here: <a href=\"https://github.com/miccunifi/ARNIQA\">https://github.com/miccunifi/ARNIQA</a></p>\n<h3>Demo Smart Video Restoration</h3>\n<p><strong>Demo</strong> of \"Smart Video Restoration\"  is available at the following link (the online demo works on photos and not videos, for timing reasons):<br>\n<a href=\"https://reinherit-image-restore.herokuapp.com/\">https://reinherit-image-restor​e.herokuapp.com</a></p>\n<p>The code of the demo is available here: <a href=\"https://github.com/ReInHerit/image_restoration\">https://github.com/ReInHerit/image_restoration</a></p>\n<p><img src=\"https://ucarecdn.com/58c061bd-e99a-4ecd-8c17-b710ac4f1987/\" alt=\"Smart Video/Photo Restoration\" title=\"Smart Video/Photo Restoration\"></p>","excerpt":"This is an application designed for historical video archive professionals that allows to restore analog videos using an innovative neural…","frontmatter":{"title":"Smart Video Restoration","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"b17b98c2-fa69-49e0-99e2-0c6111e3ca81","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/69fef94a-4726-4bae-90b9-0e1cfd24429e/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Smart.20Video.20Restoration"},"wordCount":{"words":1546}}},{"node":{"html":"<p>This is a <strong>web application</strong> that can be used to provide <strong>advanced</strong> <strong>search functions for multimedia archives.</strong> It provides <strong>content-based image retrieval (CBIR)</strong> facilities, i.e. search images based on their content. Search can be performed in two ways:</p>\n<ol>\n<li>use a <strong>textual description</strong> to search images;</li>\n<li>use a <strong>combination of reference image and associated textual description</strong> to search for images.</li>\n</ol>\n<p>The novelty of this application is due to the implementation of the computer vision component, i.e. the <strong>neural network</strong> used to associate text describing the desired content of the image and the pixels of the image. Within ReInHerit, a <strong>novel approach</strong> to perform conditioned image retrieval has been developed, i.e. the second type of search described above, that allows to <strong>search an image</strong> <strong>using an image example (a visual reference</strong>) and an <strong>additional text</strong>, expressed in natural language, that describes a modification w.r.t. the content of the reference image. The first type of image search is implemented using the same type of neural network, used in this case for uni-modal search: text-to-image retrieval (and also image-to-image retrieval). The system can be used also to perform tagging (i.e. image annotation), using a zero-shot learning approach, i.e. an approach where it is not necessary to train explicitly the network to recognize a specific content. This is extremely beneficial for <strong>small museums</strong> that may not have the large resources needed to collect training data required to train explicitly a neural network to recognize a visual concept</p>\n<p>Smart retrieval application is <strong>composed of two parts</strong>:</p>\n<ul>\n<li>a system that allows to retrieve multimedia archive materials using a <strong>single modality</strong>, i.e. the user queries that archive using a sample image or a textual description in natural language;</li>\n<li>and a <strong>multimodal approach</strong> where the user combines a sample image and additional text that requires a modification of the visual content in natural language.</li>\n</ul>\n<p>The app is a <strong>server-side application</strong> with a minimal client-side web app that shows how to use it. The server-side app must be integrated by the users.  The tool can be used both by professionals or end users in web applications. This application is designed to be desktop based (using a web interface) to be used as additional search tool in museum websites or to manage, in general, multimedia collections. It provides content-based image retrieval facilities, i.e. search images based on their content.</p>\n<p><strong>Image Classification and Content-Based Image Retrieval (CBIR)</strong> are fundamental tasks for many domains, and have been thoroughly studied by the multimedia and computer vision communities. In the cultural heritage domain, these tasks allow to <strong>simplify the management</strong> of large collections of images, allowing to annotate, search and explore them more easily and with lower costs</p>\n<p>Usually the problem of CBIR has been addressed using <strong>convolutional neural networks (CNNs)</strong>, that compute visual features representing the visual content of images and videos. Such features are then used to index and search multimedia archives. These networks are typically used in an unimodal fashion, i.e. only one media is used to train and use a network. This may limit the types of application that can be developed and may also reduce the performance of the networks. Several recent works are showing how using multi-modal approaches may improve the performance in several tasks related to visual information. It has been shown that the <a href=\"https://openai.com/research/clip\">CLIP neural network</a>, a model trained using an image-caption objective alignment on a giant dataset made of 400 million (image, text) pairs, obtains impressive results on several downstream tasks. Using only textual supervision, CLIP model learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition and many others. This task learning can be leveraged via natural language prompting to enable zero-shot transfer to many existing dataset.  CLIP was chosen as the backbone of our <strong>smart retrieval app</strong>, to solve two types of CBIR: <strong>unimodal search</strong> (text-to-image or image-to-image) and <strong>multimodal search.</strong> </p>\n<p><strong><br>\nUNIMODAL RETRIEVAL</strong><br>\nFor the first type of research, the zero-shot capabilities of CLIP in the artwork domain were exploited. The <strong>NoisyArt dataset</strong>, which is originally designed to support research on <strong>webly-supervised recognition of artworks</strong> and <strong>Zero-Shot Learning (ZSL).</strong> Webly-supervised learning is interesting since it allows to greatly reduce annotation costs required to train deep neural networks, thus allowing cultural institutions to train and develop <strong>deep learning methods</strong> while keeping their budgets for the  curation of their collections rather than the curation of training datasets. In Zero-Shot Learning approaches visual categories are acquired without any training samples, exploiting the alignment of semantic and visual information learned on some training dataset. ZSL in artwork recognition is a problem of instance recognition, unlike the other common ZSL problems that address class recognition. Zero-shot recognition is particularly appealing for cultural heritage and artwork recognition, although it is an extremely challenging problem, since it can be reasonably expected that museums have a set of curated descriptions paired with artworks in their collections. </p>\n<p>To get a better idea of how CLIP behaves in the artworks domain we started with a classification task using a shallow classifier and CLIP as the backbone.  Subsequently, thanks to the descriptions of the artworks in the dataset, we performed experiments in the field of zero-shot classification where CLIP was able to demonstrate its abilities in this task.  To evaluate the performance of our system we performed experiments on the tasks of <strong>artwork-to-artwork</strong> and <strong>description-to-artwork retrieval</strong> obtaining very promising results and superior performance to a ResNet-50 pre-trained on ImageNet, i.e. the method that we developed has a much better performance than that obtainable with neural networks designed for unimodal approaches.  The figure shows a selection of images from the NoisyArt dataset, that comprises 3.120 different classes, 70.474 images for training and 1.355 images for test.</p>\n<p><img src=\"https://ucarecdn.com/9208917f-efdb-43e9-8f5f-2ad3e80c70fc/\" alt=\"NoisyArt dataset\" title=\"NoisyArt dataset\"></p>\n<blockquote>\n<p><em>NoisyArt dataset - examples of classes and training images. For each artwork/artist pair we show the seed image obtained from DBpedia, the first two Google Image search results, and first two Flickr searches.</em></p>\n</blockquote>\n<p>To understand how CLIP associates text to the parts of the image that refers to an artwork we used a generalization of <strong>gradCAM technique</strong>, that obtains an heat-map visualization showing us the portions of the image that CLIP most closely associated with the description. </p>\n<p><img src=\"https://ucarecdn.com/fe19bca0-25ae-4aa7-bc87-7355d6aaa407/\" alt=\"Examples of such gradCAM visualization\" title=\"Examples of such gradCAM visualization\"></p>\n<blockquote>\n<p><br>\n<em>Four examples of such gradCAM visualization. We can see how, using the descriptions in the dataset, CLIP places attention to the portions of the image that are associated with the artworks shown. This fact made us confident that CLIP would work very well in the domain of artwork.</em></p>\n</blockquote>\n<p>To summarize the CBIR performance of the app we report in the following tables the comparison of several variations of use of the CLIP backbone, using text and images as keys for retrieval.  <strong>Text-to-image search</strong>: this task is akin to Zero-shot classification, we use textual queries to retrieve images, therefore we measure the performance of CLIP in terms of recognition accuracy and mean Average Precision (mAP).  <strong>Image-to-image search:</strong> in this task we compare a baseline using ResNet with variations of combinations of features extracted from CLIP. We also try to use CLIP in a zero-shot learning setup. Since the task is purely a retrieval one we report the performance only in terms of mAP. Again we show that CLIP beats a sensible baseline, but we also show how to greatly improve the basic CLIP performance using the CLIP finetuning component of the developed app, that adapts the CLIP network to datasets in the cultural heritage domain. This component will allow organizations to implement their own CBIR system beating previous state-of-the-art methods. </p>\n<p><strong>MULTIMODAL RETRIEVAL</strong><br>\nConsidering the impressive results obtained in unimodal search with CLIP, a second type of CBIR has been considered, <strong>combining text and images to express more complex queries</strong>, i.e. allowing users to represent complex visual aspects with image example and then refining their query with high-level expressions using natural language. This type of search is called, in the multimedia and computer vision community, <strong>composed image retrieval</strong>: the unimodal query is extended to an image-language pair. In a small variation, called <strong>conditioned image retrieval</strong>, the additional text may request constraints or add specifications on some attributes of the retrieved results. It must be noted that this type of search is much more complex than standard CBIR, but is receiving more attention by the scientific community since it allows to extend the effectiveness of CBIR systems by adding some form of user feedback and because it has many possible applications in different domains.  The following figure shows the concept of combined/conditioned image retrieval. </p>\n<p><img src=\"https://ucarecdn.com/e222a716-171b-4db4-9aac-51091e8bc7a4/\" alt=\"Example of combined/conditioned retrieval\" title=\"Example of combined/conditioned retrieval\"></p>\n<blockquote>\n<p><em>Example of combined/conditioned retrieval. The text provides a context to the visual query, in this case requesting to change a visual aspect of the reference image</em></p>\n</blockquote>\n<p>Since the task is much more complex than the previous one we have developed a novel neural network that combines visual and textual features computed from a CLIP backbone. This network, called <strong>combiner</strong>, learns how to transform the visual features of the reference image using the textual features of the additional query so that they become more similar to the visual features of the objects in the database. In this way, at runtime, there’s need to compute only the textual features, an operation that can be performed also on old portable PCs and run the computationally inexpensive combiner network to compute the most similar images. This approach thus allows to easily scale on large datasets, or in the case of small organizations, allow to use the system also with low power servers, or using free/low cost cloud providers. The following figure provides a gist of the architecture of the system; in order to obtain a further performance improvement we have reused the CLIP finetuning component that was developed to implement the unimodal search function presented in the previous subsection.</p>\n<p><img src=\"https://ucarecdn.com/24e8f9de-52eb-46b9-b0f2-894ac2b57de1/\" alt=\"System architecture\" title=\"System architecture\"></p>\n<blockquote>\n<p><em>System architecture of the application used for conditioned image retrieval. The CLIP image and text encoder have been finetuned with the component developed for unimodal search.</em></p>\n</blockquote>\n<p>To <strong>evaluate the performance of the system</strong> with respect to competing approaches have been used  two standard datasets employed for this task, CIRR, that contains a large variety of images, and FashionIQ that contains images related to fashion. The selection of these datasets that are commonly used by the scientific community working in this field allow us to evaluate the performance of the system fairly with respect to a large number of competing approaches. The system has the currentstate-of-the-art performance for the task of conditioned/combined image retrieval.  The application has been demonstrated at the <em>IEEE/CVF Computer Vision and Pattern Recognition Conference 2022,</em> the premier annual computer vision conference, winning the <em><a href=\"https://www.micc.unifi.it/news/acm-multimedia-2022-awards/\">Best Demo Honorable Mention award</a></em>, the most prestigious conference on computer vision.</p>\n<p><strong>WEB APP</strong><br>\nThe following figure shows an example of the web app used to test the multimodal retrieval system. The <strong>frontend</strong> is developed in <strong>Javascript</strong> and <strong>HTML5</strong>, the <strong>backend</strong> that provides the core of the application is implemented in <strong>Python</strong>, using PyTorch to implement the CV component and Flask to implement the REST services that let the interaction between the interface and the CV component.</p>\n<p><img src=\"https://ucarecdn.com/8dde4ea2-5d99-487c-bc4a-d9386e8d85d9/\" alt=\"Web interface\" title=\"Web interface\"></p>\n<blockquote>\n<p> <em>Example of the web interface of the smart retrieval app for combined CBIR. The figure shows results obtained on the CIRR dataset considering the case of search of images regarding churches.</em> </p>\n</blockquote>\n<p>A new demonstrator, implemented as a web application, has been developed, showing the <strong>text-to-image</strong> and <strong>image-to-image search</strong> capabilities on the Noisy Art dataset . Currently the system has obtained state-of-the-art results on this dataset  These two search modalities complete the text+image to image search available in the previous version of the app. The textual prompt can be expressed in natural language.</p>\n<p>The following figures show an example of the <strong>starting page,</strong> sporting a <strong>gallery view</strong> of a collection, followed by two examples of <strong>image-to-image</strong> (i.e. the visual features of a selected image are used to search similar images) and the <strong>text-to-image search,</strong> where the results are retrieved based on how much they correspond to the textual prompt.</p>\n<p><img src=\"https://ucarecdn.com/2f46ae5c-0d07-4bd3-a056-f673226107b9/\" alt=\" Smart retrieval - gallery view\" title=\" Smart retrieval - gallery view\"></p>\n<blockquote>\n<p> <em>Smart retrieval - gallery view</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/9708a996-c55f-4e72-873e-37fc064301d2/\" alt=\"Smart retrieval - image-to-image search\" title=\"Smart retrieval - image-to-image search\"></p>\n<blockquote>\n<p><em>Smart retrieval - image-to-image search</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/c67d8c30-4139-40ed-961e-dca3705dd900/\" alt=\"Smart retrieval - text-to-image search\" title=\"Smart retrieval - text-to-image search\"></p>\n<blockquote>\n<p><em>Smart retrieval - text-to-image search</em></p>\n</blockquote>\n<p>The text+image to image search function (i.e. composed image retrieval)  is currently being tested in collaboration with the <a href=\"https://fashionheritage.eu\">Europeana Fashion Heritage Association</a>, which has provided a new dataset of historical images related to fashion.</p>\n<p>Another extension of the work has addressed one of the main issues that hampers the development of composed image retrieval systems, i.e. the high effort and cost required for labeling datasets. This issue is particularly relevant in the cultural heritage domain where smaller organizations have difficulty in creating the large datasets required to train the neural networks required to address the task. This work, that has relevant scientific novelty and is currently under submission at one of the foremost conferences on computer vision <a href=\"https://iccv2023.thecvf.com\">(International Conference on Computer Vision, 2023</a>), extends conditioned image retrieval (CIR) to a zero-shot context, that does not require a labeled training dataset, thus extending the method to any dataset. The method is still based on the CLIP neural network and maps the visual features of the reference image into a pseudo-word token in CLIP token embedding space and integrates it with the relative caption. The method improves on two standard datasets that were used in our previous works (and that were until now the state-of-the-art results per scientific literature, i.e. FashionIQ and the more general CIRR dataset.</p>\n<h3>Source code</h3>\n<p>The source code of the app is available in different repositories:</p>\n<ul>\n<li>Main repository with training code: <a href=\"https://github.com/ABaldrati/CLIP4Cir\">https://github.com/ABaldrati/CLIP4Cir</a></li>\n<li>A new version of the system that is capable of zero-shot CIR, is available here: <a href=\"https://github.com/miccunifi/SEARLE\">https://github.com/miccunifi/SEARLE</a></li>\n<li>Repository of the demo system (CIR and FashionIQ): <a href=\"https://github.com/ABaldrati/CLIP4CirDemo\">https://github.com/ABaldrati/CLIP4CirDemo</a></li>\n<li>Repository of the demo system for NoisyArt dataset: <a href=\"https://github.com/ReInHerit/SmartRetrievalArtDemo\">https://github.com/ReInHerit/SmartRetrievalArtDemo</a></li>\n<li>Repository of a demo app specialzied for fashion retrieval: <a href=\"https://github.com/ReInHerit/SmartRetrievalFashion/tree/master\">https://github.com/ReInHerit/SmartRetrievalFashion/tree/master</a></li>\n</ul>","excerpt":"This is a web application that can be used to provide advanced search functions for multimedia archives. It provides content-based image…","frontmatter":{"title":"Smart Retrieval","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"e4a4ac1e-aa66-4664-a582-4055a3138d66","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/569eb601-3c1a-463d-a66b-995ac7b382a8/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Smart.20Retrieval"},"wordCount":{"words":2310}}},{"node":{"html":"<p>Strike-a-pose is a <strong>web application</strong> which performs analysis and evaluation of human poses compared to poses present in famous paintings or statues. </p>\n<p>Strike-a-Pose can be made available on the visitors' <strong>smartphone</strong>, following the “<strong>Bring Your Own Device” (BYOD</strong>) approach. The system can be used also in a dedicated environment in a gallery, using a standard PC equipped with a large screen and a camera. The same code base is used for the two setups, easing the maintenance of the application. The goal of having the application executing also on mid-level phones means that there’s no need for powerful workstations in case it is used as an installation.  The design of the interface adapts to the two different modalities, providing both a vertical interface suitable for a mobile phone and a horizontal one for installation and desktop.</p>\n<p>The application exploits a <strong>gamification paradigm</strong> with the <strong>educational</strong> purpose of getting users interested in works of art using fun. Once registered, the user is challenged to reproduce in sequence the poses of some artworks from the museum's collections. The skeleton of both the artwork and the visitor can be displayed on the screen in order to facilitate the user in matching the various points and segments. <strong>Matching the poses</strong> provides the <strong>descriptions of each artwork</strong>. The poses to be matched are organized in sets of challenges, e.g. challenges to replicate poses using the whole body, using only the torso (e.g. to allow also wheelchair users to interact), or any other type of challenge that is considered interesting by the museum curators (e.g. based on thematic collections). Once all the poses have been matched, the application allows the user to g<strong>enerate a video</strong> that can be saved for any <strong>social sharing</strong>. The video shows the user matching process and the overall interactive experience lived at the museum. The basic application can be adapted to provide variations of the gamification, e.g. introducing a competition between different users. An example of screenshots of the basic app are shown in the following figure.</p>\n<p>User is challenged to reproduce in sequence the poses of some artworks from the museum'scollections. Once all the poses have been matched, the application allows the user to generate a video that can be saved for any social sharing and provide info on the artworks.</p>\n<p><img src=\"https://ucarecdn.com/cf36b953-7d93-4e76-9747-ff76a011af30/\" alt=\"Strike a Pose WebApp \" title=\"Strike a Pose WebApp WebApp \"></p>\n<blockquote>\n<p><em>Strike a Pose WerbApp -  The user trying to strike the pose in the painting (playing in “easy\" mode, with visible skeletons -  Challenge completed: download the video.</em></p>\n</blockquote>\n<p>The application has been developed in <strong>JavaScript</strong> on the client side and in <strong>Python</strong> on the server side. Pose detection on the human bodies is achieved using <strong>TensorflowJS</strong> detection API exploiting the pose detection model, <strong>MoveNet</strong>. MoveNet is a very fast and accurate model that detects 17 key points of a body. The model is used in the variant “Lightning” intended for latency-critical applications and runs faster than real time (30+ FPS) on most modern desktops, laptops, and phones. The model runs completely client-side in the browser; this allows us to run the whole computer vision task on the device of the user, providing a better user experience thanks to the reduced latency for the pose analysis. Server-side an SQLLite database is used to store artworks' collections, challenges and artworks' metadata and descriptions. Communication between the knowledge-base and the interface is ensured through RESTful APIs developed in Flask. The video is created server side. </p>\n<p>The base interface, implemented in <strong>HTML</strong> can be adapted by different users, maintaining the computer vision functionalities, so as to allow customization by different museums.  The web interface is designed to be responsive and adapt to different devices.</p>\n<p>Use state-of-the-art AI techniques that can run on mobile devices, to follow BYOD approach. App doesn’t ask for personal data, no logging of personal data, performing as much as possible computation on end-user devices, as explained in the <strong>Privacy Policy .</strong></p>\n<p><img src=\"https://ucarecdn.com/b0886b3b-af0a-486d-9b42-a8d769cb6a89/\" alt=\"Strike a Pose - Privacy Policy\" title=\"Strike a Pose - Privacy Policy\"></p>\n<blockquote>\n<p><em>Strike a Pose - Privacy Policy</em></p>\n</blockquote>\n<p><strong>User</strong> is able to <strong>create a room</strong> by selecting the number of participants (clients) i.e. whether to interact alone (Solo) or a challange with another user (Versus), setting the number of poses the number of rounds and the level of half bust or full body of the artwork. Once the room is created, user can share the id number to invite participants.</p>\n<p><img src=\"https://ucarecdn.com/d965100e-a971-4662-9e15-3dcc92e50be7/\" alt=\"Strike a Pose - Setting the room\" title=\"Strike a Pose - Setting the room\"></p>\n<blockquote>\n<p><em>Strike a Pose - Setting the room</em></p>\n</blockquote>\n<p><strong>Museum</strong> is able to access the <strong>Admin Dashboard</strong> to directly manage user interaction artworks.</p>\n<p><img src=\"https://ucarecdn.com/1cf5540d-ecfa-4fdc-848b-58b5df986b1a/\" alt=\"Strike-a-pose - Admin Dashboard\" title=\"Strike-a-pose - Admin Dashboard\"></p>\n<blockquote>\n<p><em>Strike-a-pose - Admin Dashboard</em></p>\n</blockquote>\n<h3><strong>Usage Example</strong></h3>\n<p>App requires a server to host the mobile app and to provide the RESTful APIs of the backend. A QR code can be used to avoid typing the URL of the web apps.</p>\n<p>The application is composed of a <strong>backend</strong> that manages the challenges and a <strong>front-end</strong> that runs on mobile devices. A schema of the <strong>main components</strong> of the <strong>backend and front end</strong>  is shown in the following figures:</p>\n<p><img src=\"https://ucarecdn.com/65a51251-857f-413e-9fa6-f1ce4e42dd8b/\" alt=\"Main components of the backend and front-end of Strike-a-pose\" title=\"Main components of the backend and front-end of Strike-a-pose\"></p>\n<blockquote>\n<p><em>Main components of the backend and front-end of Strike-a-pose</em></p>\n</blockquote>\n<p>The interface is completely written in <strong>HTML5</strong>. The computer vision task of matching the pose of the user with the pose of the artworks of the challenge can be implemented completely using the <strong>Tensorflow JS</strong> executed in the browser. The creation of the video produced upon successful completion of the challenge is performed server-side.  The <strong>backend</strong> implements a RESTful API using Flask, and is fully implemented in Python. This eases the integration of several other libraries to access the database of the challenges, using SQLAlchemy and to create the final videos using OpenCV**.** Challenges are maintained in the DB, e.g. allowing to pick poses that require to use only the upper body (e.g. to allow also people with mobility issues to play) or with full body. </p>\n<p>jQuery and Bootstrap are the main components used to design the interface and provide user interaction with the GUI, Webcam Easy JS allows to connect to the webcam through the browser and Tensorflow JS is the workhorse to implement the computer vision functionalities.</p>\n<h3><strong>Guidelines for reuse</strong></h3>\n<p>The simplest type of reuse is substituting the selected sample artworks with those of the collection of the museum/organization that desires to customize the apps, along with the associated information. Setup of the apps is based on Docker, to simplify the installation of the backend. </p>\n<p>It is possible to extend the apps introducing new types of challenges, e.g. combining classes of artworks, creating collections of artworks according to some criterion. The challenges of Strike-a-pose can be changed to follow some other criterion other than using full/upper/lower body parts, e.g. according to styles or time. </p>\n<p>Changing the GUIs is relatively easy, since it is needed to update the HTML5 of Strike-a-pose or the Kivy code of Face-fit.</p>\n<h3>Source Code</h3>\n<p><strong>Free Codes of the App are available here:</strong><br>\n<a href=\"https://github.com/ReInHerit/strike-a-pose\">https://github.com/ReInHerit/strike-a-pose</a></p>\n<h3>Demo</h3>\n<p><strong>Demo</strong> is available at this link (using Chrome or Firefox browser): :<br>\n<a href=\"https://reinherit-strike-a-pose.herokuapp.com/\">https://reinherit-strike-a-pos​e.herokuapp.com/</a></p>\n<p>For more information and support contact <a href=\"\">marco.bertini@unifi.it  </a>MICC - <a href=\"http://www.micc.unifi.it\">Media Integration and Communication Cente</a>r, University of Florence,  Italy</p>\n<p>\n        <div class=\"embedVideo-container\">\n            <iframe\n              title=\"\"\n              width=\"560\"\n              height=\"316\"\n              src=\"https://www.youtube.com/embed/GHgBIRXqKK8?rel=0\"\n              class=\"embedVideo-iframe\"\n              style=\"border:0\"\n              \n              loading=\"eager\"\n              allowfullscreen\n\t      sandbox=\"allow-same-origin allow-scripts allow-popups\"\n            ></iframe>\n        </div></p>","excerpt":"Strike-a-pose is a web application which performs analysis and evaluation of human poses compared to poses present in famous paintings or…","frontmatter":{"title":"Strike a Pose","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"5f367b50-4089-4718-9b66-8114962c6596","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/bde51a50-8ac7-4bf9-aa71-c2699d7c2865/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Strike.20a.20pose"},"wordCount":{"words":1174}}},{"node":{"html":"<p><strong>Face-fit</strong> follows the idea of the Toolkit's <a href=\"https://reinherit-hub.eu/tools/apps/5f367b50-4089-4718-9b66-8114962c6596\">Strike-a-pose</a> app, i.e. asking the user to <strong>replicate an artwork</strong>, but concentrating on <strong>facial expressions</strong> that require a much more refined matching. </p>\n<p>This application can be used on a <strong>mobile phone</strong> or on a PC, but due to technical limitations of some required computer vision libraries the <strong>Javascript</strong> version for mobile phones need to relieve some image processing functionality to a server. For this reason the app has been developed using two codebases: a <strong>Javascript</strong> one, with <strong>TensorflowJS</strong>, for mobile phones and a <strong>Python</strong> version using <strong>OpenCV</strong> and Tensorflow for the desktop app for museum installations. </p>\n<p><img src=\"https://ucarecdn.com/3726575e-35ed-4268-b742-0060e249e09f/\" alt=\"Face-Fit App Interface  \" title=\"Face-Fit App Interface  \"></p>\n<blockquote>\n<p><em>Face-Fit App Interface</em></p>\n</blockquote>\n<p>The application asks the users to <strong>replicate the pose</strong> of the <strong>head</strong> and the <strong>expression</strong> of some <strong>portraits</strong> by famous painters and transfer the face of the user on the artworks, generating a new image. The application was designed through a usability study carried out following an iterative design approach with three groups of 5 people. The user places himself in front of the <strong>smartphone or installation equipped with a camera</strong>. He is presented with a series of portraits' paintings in a vertical carousel. The user can choose the artwork to match. At that point the application presents a ghost image of the user's face that the user must try to super-impose on that of the painting to find a perfect match, see following figure.</p>\n<p>App doesn’t ask for personal data, no logging of personal data, performing as much as possible computation on end-user devices, as explained in the <strong>Privacy Policy .</strong></p>\n<p><img src=\"https://ucarecdn.com/8901ee1f-fc4c-449b-bbdd-8532543e038f/\" alt=\"Face-Fit - Privacy Policy\" title=\"Face-Fit - Privacy Policy\"></p>\n<blockquote>\n<p><em>Face-Fit -</em>  <em>Privacy Policy</em></p>\n</blockquote>\n<p>The <strong>ghost image solution</strong> was the result of our usability study which solved some issues related to how to keep the user at the same time concentrated on the task without losing the fun of the game. At first, in fact, we had provided some visual suggestions to find the right pose but they distracted the user from the painting and therefore from the game.</p>\n<p><img src=\"https://ucarecdn.com/ae2eaee6-9357-49b2-b41e-1cea3fa951ce/\" alt=\"User interaction\" title=\"User interaction\"></p>\n<blockquote>\n<p><em>User interaction: the ghost image gives feedback to the user to change his pose and expression to better match that of the artwork.</em> <em>The “ghost” image is the face of the user superimposed on the artwork.</em></p>\n</blockquote>\n<p>A faster than real-time face mesh prediction network is used to obtain 468 3D points for each face, also when using mobile phones.  The points are used to compute the pose of the whole face. Once the pose is matched, the position of eyes, eyebrows and mouth is matched. When both pose and facial expression match, the face of the user is substituted to that of the painting and the description of the artwork is provided. </p>\n<p>Once the pose is matched the user obtains <strong>information on the artwork</strong> and can download the <strong>generated images for sharing on social networks.</strong> By entering your email, the system automatically sends images and in-depth content about the artworks managed by the museum. The user's data and email are not stored and are deleted from the system as soon as the email is sent, in accordance with User Data privacy.</p>\n<p><img src=\"https://ucarecdn.com/88a65fb9-6b1e-497a-8d81-cd68f2bdd79f/\" alt=\"In-depth content and generated images sent via email for sharing on social networks.\" title=\"In-depth content and generated images sent via email for sharing on social networks.\"></p>\n<blockquote>\n<p><em>In-depth content and generated images sent via email for sharing on social networks.</em></p>\n</blockquote>\n<p><strong>Museum</strong> is able to access the <strong>Admin Dashboard</strong>  adding the artworks interacted with by users, and managing content associated with the artwork and to share with the user for more in-depth analysis of the artworks.</p>\n<p><img src=\"https://ucarecdn.com/998cd54f-7045-40c4-81e4-41dd637b929d/\" alt=\"Face Fit - Admin Dashboard\" title=\"Face Fit - Admin Dashboard\"></p>\n<blockquote>\n<p><em>Face Fit - Admin Dashboard</em></p>\n</blockquote>\n<p>Differently from Strike-a-pose the mobile version of the Face-fit application has some differences from the stand-alone version. This is due to the need to use some OpenCV  functions, to perform the color correction of the images that allow to change the style of the images captured from the webcam to the style of the painting, that are not available in Javascript. This has required to implement these functionalities as RESTful APIs in Python, while the frontend is implemented in Javascript with Tensorflow JS, porting the Python code of the standalone version. </p>\n<p>The Face Mesh neural network used is based on a MobileNetV2 architecture that can run in real-time also on mid-level mobile phones.</p>\n<h3><strong>Usage Example</strong></h3>\n<p>App requires a server to host the mobile app and to provide the RESTful APIs of the backend. A QR code can be used to avoid typing the URL of the web apps.</p>\n<h3>Guidelines for reuse</h3>\n<p>The simplest type of reuse is substituting the selected sample artworks with those of the collection of the museum/organization that desires to customize the apps, along with the associated information. Setup of the apps is based on Docker, to simplify the installation of the backend. </p>\n<p>It is possible to extend the apps introducing new types of challenges, e.g. combining classes of artowks in Face-fit as it is done in Strike-a-pose, creating collections of artworks according to some criterion. The challenges of Strike-a-pose can be changed to follow some other criterion other than using full/upper/lower body parts, e.g. according to styles or time. </p>\n<p>Changing the GUIs is relatively easy, since it is needed to update the HTML5 of Strike-a-pose or the Kivy code of Face-fit.</p>\n<h3><strong>Source code</strong></h3>\n<p><strong>Free Codes of the App are available here:</strong><br>\n<a href=\"https://github.com/ReInHerit/face-fit\">https://github.com/ReInHerit/face-fit</a></p>\n<h3>Demo</h3>\n<p><strong>Demo</strong> is available at this link  (using Chrome or Firefox browser): <br>\n<a href=\"https://reinherit-facefit.herokuapp.com/\">https://reinherit-facefit.hero​kuapp.com</a> </p>\n<p>For more information and support contact <a href=\"\">marco.bertini@unifi.it  </a>MICC - <a href=\"http://www.micc.unifi.it\">Media Integration and Communication Center</a>, University of Florence,  Italy</p>\n<p>\n        <div class=\"embedVideo-container\">\n            <iframe\n              title=\"\"\n              width=\"560\"\n              height=\"316\"\n              src=\"https://www.youtube.com/embed/GHgBIRXqKK8?rel=0\"\n              class=\"embedVideo-iframe\"\n              style=\"border:0\"\n              \n              loading=\"eager\"\n              allowfullscreen\n\t      sandbox=\"allow-same-origin allow-scripts allow-popups\"\n            ></iframe>\n        </div></p>","excerpt":"Face-fit follows the idea of the Toolkit's Strike-a-pose app, i.e. asking the user to replicate an artwork, but concentrating on facial…","frontmatter":{"title":"Face-Fit","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"7ae4d04e-fc32-48b0-b2fe-6c035d28bde8","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/94e6f411-b4ab-4c44-9322-5966136b0ec4/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Face.20Fit"},"wordCount":{"words":907}}},{"node":{"html":"<p><strong>Multimedia Chatbot</strong>  is a <strong>web application</strong> that can be integrated within a museum website or as a mobile web site, and provides an <strong>interaction based on natural language processing and chat</strong>. Users can either type their questions or interact with speech that is then translated to text. This <strong>web-based application</strong> implements a <strong>chatbot</strong> <strong>system</strong> that can answer questions about <strong>visual content</strong> of artworks or about their <strong>context</strong>, e.g. about the author and history of the artwork. The design of this application is motivated by the recent huge interest in chat-based interaction that has been popularized, for example, by ChatGPT.</p>\n<p><strong>Examples of the interface of the multimedia chatbot</strong> are shown in the following Images.</p>\n<p><img src=\"https://ucarecdn.com/e5541477-fcb5-4333-9361-e612c629fc57/\" alt=\"VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork\" title=\"VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork\"></p>\n<p><img src=\"https://ucarecdn.com/d62ead48-95c6-4d4b-96d9-1c5d1d68805e/\" alt=\"VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork\" title=\"VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork\"></p>\n<blockquote>\n<p><em>VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/e2b58a76-85c9-4892-8c51-ec1e044b2bad/\" alt=\"VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork\" title=\"VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork\"></p>\n<blockquote>\n<p><em>VIOLA multimedia chatbot: Gallery and  Answering questions related to the context of an artwork</em></p>\n</blockquote>\n<p>The content used by the chatbot is  provided by curators using a specific <strong>ADMIN interface.</strong></p>\n<p><img src=\"https://ucarecdn.com/57de6711-8598-4307-822c-00bd8fcc4e84/\" alt=\"VIOLA multimedia chatbot: ADMIN Interface for curators\" title=\"VIOLA multimedia chatbot: ADMIN Interface for curators\"></p>\n<p><em>VIOLA multimedia chatbot: ADMIN Interface for curators</em></p>\n<p><img src=\"https://ucarecdn.com/7aed416f-ad1a-468c-8d69-d7a33acd4891/\" alt=\"VIOLA multimedia chatbot: ADMIN Interface for curators\" title=\"VIOLA multimedia chatbot: ADMIN Interface for curators\"></p>\n<p>The application <strong>frontend</strong> is based on <strong>Javascript</strong>, and the interface is based on a reactive design that adapts both to desktop web browsers, allowing to add this functionality to the existing websites of museums, or to a mobile application to provide a new type of smart guide. To ease this latter type of interaction, queries of the users can be performed using speech recognition, so to avoid typing long queries on the small keyboard of the device.</p>\n<p>The <strong>backend</strong> is implemented in <strong>Python</strong>, using <strong>Flask</strong> to provide the REST API to the <strong>frontend</strong>.</p>\n<p>There are <strong>two different versions of the backend;</strong> one implements a set of three neural networks:</p>\n<ul>\n<li>a <strong>neural network</strong> <em><strong>classifies</strong></em> the type of the query of the user understanding if it is about the visual content or the context of th artwork;</li>\n<li>a <strong>neural network</strong> for <em><strong>question answering (QA)</strong></em> uses the contextual information of the artwork, stored as JSON data, to answer questions about the the context of the artwork;</li>\n<li>a <strong>neural network</strong> for <em><strong>visual question answering</strong></em> considers the visual data of the image and the visual description of the artwork, stored as JSON data, to answer questions about the content of the artwork.</li>\n</ul>\n<p>The idea of this system is to overcome the limitations of existing visual question answering (VQA) approaches, that take as input an image and a question about the image content and aim to answer correctly to the input question (see following figure). In fact VQA systems are limited in that they:</p>\n<ul>\n<li><strong>are able to</strong> answer questions about the <strong>image content (visual questions)</strong> with a few words;</li>\n<li><strong>are not able</strong> to answer questions about the image which involve <strong>external information (contextual questions)</strong> not inferable from the image content.     </li>\n</ul>\n<p>However, in the Cultural Heritage domain contextual questions are very frequent (<em>when was the painting depicted?... who is the author?</em>...)</p>\n<p>The design of the first type of chatbot implemented in the Multimedia chatbot application follows thus the <strong>schema</strong> represented in the next figure.</p>\n<p><img src=\"https://ucarecdn.com/133da573-fb5b-4438-9973-b1c80e0c12e5/\" alt=\"Multimedia chatbot: example of visual question answering (VQA) for cultural heritage, answering a question related to the context of the artwork\" title=\"Multimedia chatbot: example of visual question answering (VQA) for cultural heritage, answering a question related to the context of the artwork\"></p>\n<blockquote>\n<p><em>Multimedia chatbot: example of visual question answering (VQA) for cultural heritage, answering a question related to the context of the artwork</em></p>\n</blockquote>\n<p><img src=\"https://ucarecdn.com/1e0e1087-2964-49ce-900c-235bb2475c84/\" alt=\"multimedia chatbot: example of visual question answering (VQA) for cultural heritage, answering a question related to the content of the artwork\" title=\"multimedia chatbot: example of visual question answering (VQA) for cultural heritage, answering a question related to the content of the artwork\"></p>\n<blockquote>\n<p><em>Multimedia chatbot: example of visual question answering (VQA) for cultural heritage, answering a question related to the content of the artwork</em></p>\n</blockquote>\n<p>The question classifier network is a network that processes only the textual information given by the question. It has the structure of a <strong>Transformer model</strong> followed by a classification head, and has been trained on questions of both the VQA v22 and OK-VQA3 datasets.</p>\n<p>The <strong>VQA network</strong> extracts the salient region features of the image using Faster-RCNN4 (pretrained on the Visual Genome5 Dataset). It uses an attention mechanism to filter the image regions according to the input question and has been trained on examples of VQA v2 dataset.</p>\n<p>The <strong>QA network</strong> uses an attention mechanism to find the answer to the question in the text. It has the structure of the Transformer models and has been  trained on Squad6 dataset.</p>\n<p>The following figures show some qualitative results obtained by the system, highlighting in red the mistakes</p>\n<p><img src=\"https://ucarecdn.com/989fc728-3eda-475b-af05-c10e599baa58/\" alt=\" multimedia chatbot: qualitative results of the question classifier, question answering network and visual question answering network.\" title=\" multimedia chatbot: qualitative results of the question classifier, question answering network and visual question answering network.\"></p>\n<blockquote>\n<p> <em>Multimedia chatbot: qualitative results of the question classifier, question answering network and visual question answering network.</em></p>\n</blockquote>\n<p>In addition to this first type of chatbot system, following the emergence and success of neural networks based on <strong>GPT architectures</strong> and training, a second chatbot engine has been added to the backend, using a <strong>GPT-based neural network</strong>. A paper describing the use of this type of neural network has been published in a workshop dedicated to applications of AI and computer vision to cultural heritage in one of the foremost conferences on computer vision (<a href=\"https://eccv2022.ecva.net\">European Conference on Computer Vision 2022</a>) This second system allows to create longer answers compared to the first approach, thus creating a more natural interaction. To cope with the fact that <strong>GPT-based system</strong> tend to create texts that sound plausible only from a linguistic point of view, but that are not based on actual knowledge, the system has been designed using prompt-engineering techniques that force the neural network to adhere to the contextual and content-based information provided by the JSON files that contain actual details and information related to the artwork.</p>\n<p><strong>A first Alpha version of the VIOLA Multimedia Chatbot</strong> has been created by importing a dataset called <a href=\"https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=35\">Artpedia</a> that includes numerous images and descriptions and is commonly used in the computer vision community to test Visual Question Answering (VQA) systems.  This version was only an Alpha phase of demonstration testing with a large gallery of images.  To resolve ethical issues related to the data accuracy.  </p>\n<p>\n        <div class=\"embedVideo-container\">\n            <iframe\n              title=\"\"\n              width=\"560\"\n              height=\"316\"\n              src=\"https://www.youtube.com/embed/BjstSqyxbOg?rel=0\"\n              class=\"embedVideo-iframe\"\n              style=\"border:0\"\n              \n              loading=\"eager\"\n              allowfullscreen\n\t      sandbox=\"allow-same-origin allow-scripts allow-popups\"\n            ></iframe>\n        </div></p>\n<p><strong>A new version of the multimedia chatbot VIOLA</strong> has been implemented. The number of images are significantly reduced to show only a gallery of images related to a limited selection of artworks provided by the Museums involved in the ReinHerit Project (Graz Museum, Cycladic Museum, Bank of Cyprus Foundation). Museum curators and experts from the ReInHerit Consortium have provided quality content on some of the artworks in these museums. The related data helped validate the chatbot results, ensuring that the system's responses are in line with scientific accuracy. A final backend interface was developed for museum curators, where they can add and edit artwork data, including with the ability to upload code via a JSON file. </p>\n<p>\n        <div class=\"embedVideo-container\">\n            <iframe\n              title=\"\"\n              width=\"560\"\n              height=\"316\"\n              src=\"https://www.youtube.com/embed/lVeM2oazqvA?rel=0\"\n              class=\"embedVideo-iframe\"\n              style=\"border:0\"\n              \n              loading=\"eager\"\n              allowfullscreen\n\t      sandbox=\"allow-same-origin allow-scripts allow-popups\"\n            ></iframe>\n        </div></p>\n<p><strong>The chatbot works this way</strong>: the questions of the users are encapsulated within a set of instructions that force the chatbot to answer only using the quality content about the artwork. This solution will seek to avoid errors by relying on quality content provided by curators, through the development of appropriate prompts capable of directing responses to the validated dataset collected. The more the textual description the better the chatbot will answer the questions of the users.</p>\n<h3>Source code</h3>\n<p>The source code of the app is available on the GitHub of ReInHerit: <a href=\"https://github.com/ReInHerit/multimedia-chatbot\">https://github.com/ReInHerit/multimedia-chatbot</a></p>\n<h3>Demo</h3>\n<p>The DEMO version of the Chatbot includes a gallery with a limited selection of artworks validated by the museums participating in the ReinHerit project (<a href=\"https://www.grazmuseum.at/en/\">GrazMuseum</a>, <a href=\"https://cycladic.gr/en/\">Museum of Cycladic Art</a>,<br>\n<a href=\"https://www.boccf.org/en-gb/homepage/\">Bank of Cyprus Cultural Foundation</a>). Museum curators provided images and related content. They validated the descriptive content of the artworks to ensure quality responses. Unanswered questions will be saved to add more descriptive content and improve dialogue and chatbot interaction</p>\n<p>Additionally, we are working with curators of the following small and medium-sized museums to test and add more artworks: <a href=\"https://www.museisenesi.org/en/\">Fondazione Musei Senesi</a> IT, <a href=\"https://www.gipsoteca.sma.unipi.it/en/\">Giara Gipsoteca di Arte Antica e Antiquarium University of Pisa</a> IT, <a href=\"https://www.musei.re.it/musei2021/\">Musei Civici di Reggio Emilia</a> IT, <a href=\"http://www.museifoligno.it/i-musei/museo-capitolare-diocesano\">Museo Capitolare Diocesano Foligno</a> IT</p>\n<p>Demo is available at this link:<br>\n<a href=\"https://reinherit-multimedia-chatbot.herokuapp.com/\">https://reinherit-multimedia-c​hatbot.herokuapp.com/</a></p>","excerpt":"Multimedia Chatbot  is a web application that can be integrated within a museum website or as a mobile web site, and provides an interaction…","frontmatter":{"title":"Multimedia Chatbot","date":"a year ago","target_audience":["PROFESSIONAL"],"layout":null,"type":"toolapp","pageId":"c01cc7e5-033c-4d07-a56f-4612f9f210b3","license":"CC BY 2.0","thumbnail":"https://ucarecdn.com/c9c5d1cb-997e-4111-ba9c-49d9b1690bf8/","chatApps":"https://reinherit.zulipchat.com/#narrow/stream/392282-ReInHerit-Applications-and-Toolkit/topic/Multimedia.20Chatbot.20VIOLA"},"wordCount":{"words":1365}}}]}}}